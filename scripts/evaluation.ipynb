{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd7977-ef09-4169-83d8-b30c092d51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import (\n",
    "    EsmTokenizer,\n",
    "    EsmForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel\n",
    ")\n",
    "from peft import PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df527262-5820-4e02-87db-d4d814a6a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Definition\n",
    "class Cosine(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return nn.CosineSimilarity(dim=-1)(x1, x2)\n",
    "\n",
    "class SquaredCosine(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return nn.CosineSimilarity(dim=-1)(x1, x2) ** 2\n",
    "\n",
    "class Euclidean(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cdist(x1, x2, p=2.0)\n",
    "\n",
    "class SquaredEuclidean(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cdist(x1, x2, p=2.0) ** 2\n",
    "\n",
    "DISTANCE_METRICS = {\n",
    "    \"Cosine\": Cosine,\n",
    "    \"SquaredCosine\": SquaredCosine,\n",
    "    \"Euclidean\": Euclidean,\n",
    "    \"SquaredEuclidean\": SquaredEuclidean,\n",
    "}\n",
    "\n",
    "class Coembedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        molecule_shape: int = 768,\n",
    "        protein_shape: int = 1280,\n",
    "        latent_dimension: int = 1024,\n",
    "        latent_activation=nn.ReLU,\n",
    "        latent_distance: str = \"Cosine\",\n",
    "        classify: bool = True,\n",
    "        temperature: float = 0.1\n",
    "    ):\n",
    "        super(Coembedding, self).__init__()\n",
    "        self.molecule_shape = molecule_shape\n",
    "        self.protein_shape = protein_shape\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.do_classify = classify\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "\n",
    "        self.molecule_projector = nn.Sequential(\n",
    "            nn.Linear(self.molecule_shape, latent_dimension),\n",
    "            latent_activation(),\n",
    "            nn.Linear(latent_dimension, latent_dimension)\n",
    "        )\n",
    "        \n",
    "        for layer in self.molecule_projector:\n",
    "            if isinstance(layer, nn.Linear): \n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "        self.protein_projector = nn.Sequential(\n",
    "            nn.Linear(self.protein_shape, latent_dimension),\n",
    "            latent_activation(),\n",
    "            nn.Linear(latent_dimension, latent_dimension)\n",
    "        )\n",
    "        \n",
    "        for layer in self.protein_projector:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "        if self.do_classify:\n",
    "            if latent_distance not in DISTANCE_METRICS:\n",
    "                raise ValueError(f\"Unsupported distance metric: {latent_distance}\")\n",
    "            self.distance_metric = latent_distance\n",
    "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
    "\n",
    "    def forward(self, molecule, protein):\n",
    "        if self.do_classify:\n",
    "            return self.classify(molecule, protein)\n",
    "        else:\n",
    "            return self.regress(molecule, protein)\n",
    "\n",
    "    def regress(self, molecule, protein):\n",
    "        molecule_projection = self.molecule_projector(molecule)\n",
    "        protein_projection = self.protein_projector(protein)\n",
    "\n",
    "        inner_prod = torch.bmm(\n",
    "            molecule_projection.view(-1, 1, self.latent_dimension),\n",
    "            protein_projection.view(-1, self.latent_dimension, 1),\n",
    "        ).squeeze()\n",
    "        relu_f = nn.ReLU()\n",
    "        return relu_f(inner_prod).squeeze()\n",
    "\n",
    "    def classify(self, molecule, protein):\n",
    "        molecule_projection = self.molecule_projector(molecule)\n",
    "        protein_projection = self.protein_projector(protein)\n",
    "\n",
    "        molecule_projection = molecule_projection.unsqueeze(0) \n",
    "        protein_projection = protein_projection.unsqueeze(1) \n",
    "\n",
    "        distance = self.activator(molecule_projection, protein_projection)\n",
    "        \n",
    "        scaled_distance = distance / self.temperature\n",
    "\n",
    "        return scaled_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4028a-18b4-41bd-9685-a5231de48790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_embeddings_batch(sequences, tokenizer, lora_model, device):\n",
    "    lora_model.to(device)\n",
    "    inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = lora_model.esm(**inputs).last_hidden_state\n",
    "        mean_output = output[:, 1:output.size(1)].mean(dim=1)\n",
    "    return mean_output.cpu()\n",
    "\n",
    "def generate_mol_embeddings_batch(smiles_list, tokenizer, mol_model, device, target_dim=1280):\n",
    "    try:\n",
    "        mol_model.to(device)\n",
    "        inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = mol_model(**inputs)\n",
    "            mol_embedding = outputs.pooler_output\n",
    "        return mol_embedding.cpu()  # Move to CPU only after computation\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SMILES: {smiles_list}, Error: {e}\")\n",
    "        return torch.zeros((len(smiles_list), target_dim))  # Return zero tensor for invalid SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba38a8-6e70-46b6-a54d-c41d0b52bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'esm2/esm2_t33_650M_UR50D'\n",
    "prot_tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "base_model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "prot_model = PeftModel.from_pretrained(base_model, './plm')\n",
    "\n",
    "mol_model_path = \"./ibm/MoLFormer-XL-both-10pct\"\n",
    "mol_tokenizer = AutoTokenizer.from_pretrained(mol_model_path, trust_remote_code=True)\n",
    "mol_model = AutoModel.from_pretrained(mol_model_path, deterministic_eval=True, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Coembedding().to(device)\n",
    "model.load_state_dict(torch.load('model_weight/best_model.pth')['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "data = pd.read_excel(\"T2_data_normalized.xlsx\")\n",
    "data = data.drop_duplicates(subset = ['canonicalsmiles'])\n",
    "\n",
    "# Dataset split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract sequences and SMILES from the split test set\n",
    "test_prot_seq = test_data.sequence.tolist()\n",
    "mol_smiles = data.canonicalsmiles.tolist()\n",
    "\n",
    "unique_labels = data.T2PKproductsname.tolist()\n",
    "label_to_index = {productsname: idx for idx, productsname in enumerate(unique_labels)}\n",
    "index_to_label = {idx: productsname for productsname, idx in label_to_index.items()}\n",
    "true_labels = test_data['T2PKproductsname'].map(label_to_index).tolist()\n",
    "\n",
    "# Function to calculate top-k accuracy\n",
    "def calculate_topk_accuracy(model, mol_emb, prot_emb, true_labels, k=3):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(mol_emb, prot_emb)\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        _, top_k_preds = torch.topk(outputs, k, dim=1)\n",
    "        top_k_preds = top_k_preds.cpu().numpy()\n",
    "        \n",
    "        # Initialize accuracy counter\n",
    "        accuracy_count = 0\n",
    "        \n",
    "        # Iterate through each sample's predictions and true labels\n",
    "        for i in range(len(true_labels)):\n",
    "            if true_labels[i] in top_k_preds[i]:  # If true label is in top-k predictions\n",
    "                accuracy_count += 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_count / len(true_labels)\n",
    "        return accuracy\n",
    "\n",
    "# Generate embeddings for training and test sets\n",
    "test_prot_emb = generate_anchor_embeddings_batch(test_prot_seq, prot_tokenizer, prot_model, device).to(device)\n",
    "mol_emb = generate_mol_embeddings_batch(mol_smiles, mol_tokenizer, mol_model, device).to(device)\n",
    "\n",
    "# Calculate accuracy for different k values\n",
    "test_top1_accuracy = calculate_topk_accuracy(model, mol_emb, test_prot_emb, true_labels, k=1)\n",
    "test_top3_accuracy = calculate_topk_accuracy(model, mol_emb, test_prot_emb, true_labels, k=3)\n",
    "test_top5_accuracy = calculate_topk_accuracy(model, mol_emb, test_prot_emb, true_labels, k=5)\n",
    "\n",
    "print(f\"Test Top-1 Accuracy: {test_top1_accuracy:.4f}\")\n",
    "print(f\"Test Top-3 Accuracy: {test_top3_accuracy:.4f}\") \n",
    "print(f\"Test Top-5 Accuracy: {test_top5_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fc950-e622-46f9-ac19-6a6f5f0eed23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
