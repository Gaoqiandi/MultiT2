{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10e64b-5505-45e1-8397-dd5abd554355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import (\n",
    "    EsmTokenizer,\n",
    "    EsmForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel\n",
    ")\n",
    "from peft import PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a4937-456c-41e3-ba0c-45cea282b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss:\n",
    "    def __init__(self, temperature=0.1):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, anchor_projection, positive_projection):\n",
    "        return self.loss_fn(anchor_projection, positive_projection)\n",
    "\n",
    "    def loss_fn(self, anchor_projection, positive_projection):\n",
    "        similarity_matrix = torch.matmul(anchor_projection, positive_projection.T)\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "        \n",
    "        pos_sim = torch.diag(similarity_matrix)\n",
    "\n",
    "        lprobs_pocket = F.log_softmax(similarity_matrix, dim=1)\n",
    "        indices = torch.arange(len(pos_sim))\n",
    "        L_pocket = -lprobs_pocket[indices, indices].mean()\n",
    "\n",
    "        lprobs_mol = F.log_softmax(similarity_matrix.T, dim=1)\n",
    "        L_mol = -lprobs_mol[indices, indices].mean()\n",
    "\n",
    "        loss = 0.5 * (L_pocket + L_mol)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Distance Definition\n",
    "class Cosine(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return nn.CosineSimilarity(dim=-1)(x1, x2)\n",
    "\n",
    "class SquaredCosine(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return nn.CosineSimilarity(dim=-1)(x1, x2) ** 2\n",
    "\n",
    "class Euclidean(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cdist(x1, x2, p=2.0)\n",
    "\n",
    "class SquaredEuclidean(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cdist(x1, x2, p=2.0) ** 2\n",
    "\n",
    "DISTANCE_METRICS = {\n",
    "    \"Cosine\": Cosine,\n",
    "    \"SquaredCosine\": SquaredCosine,\n",
    "    \"Euclidean\": Euclidean,\n",
    "    \"SquaredEuclidean\": SquaredEuclidean,\n",
    "}\n",
    "\n",
    "class Coembedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        molecule_shape: int = 768,\n",
    "        protein_shape: int = 1280,\n",
    "        latent_dimension: int = 1024,\n",
    "        latent_activation=nn.ReLU,\n",
    "        latent_distance: str = \"Cosine\",\n",
    "        classify: bool = True,\n",
    "        temperature: float = 0.1\n",
    "    ):\n",
    "        super(Coembedding, self).__init__()\n",
    "        self.molecule_shape = molecule_shape\n",
    "        self.protein_shape = protein_shape\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.do_classify = classify\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "\n",
    "        self.molecule_projector = nn.Sequential(\n",
    "            nn.Linear(self.molecule_shape, latent_dimension),\n",
    "            latent_activation(),\n",
    "            nn.Linear(latent_dimension, latent_dimension)\n",
    "        )\n",
    "        \n",
    "        for layer in self.molecule_projector:\n",
    "            if isinstance(layer, nn.Linear): \n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "        self.protein_projector = nn.Sequential(\n",
    "            nn.Linear(self.protein_shape, latent_dimension),\n",
    "            latent_activation(),\n",
    "            nn.Linear(latent_dimension, latent_dimension)\n",
    "        )\n",
    "        \n",
    "        for layer in self.protein_projector:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "        if self.do_classify:\n",
    "            if latent_distance not in DISTANCE_METRICS:\n",
    "                raise ValueError(f\"Unsupported distance metric: {latent_distance}\")\n",
    "            self.distance_metric = latent_distance\n",
    "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
    "\n",
    "    def forward(self, molecule, protein):\n",
    "        if self.do_classify:\n",
    "            return self.classify(molecule, protein)\n",
    "        else:\n",
    "            return self.regress(molecule, protein)\n",
    "\n",
    "    def regress(self, molecule, protein):\n",
    "        molecule_projection = self.molecule_projector(molecule)\n",
    "        protein_projection = self.protein_projector(protein)\n",
    "\n",
    "        inner_prod = torch.bmm(\n",
    "            molecule_projection.view(-1, 1, self.latent_dimension),\n",
    "            protein_projection.view(-1, self.latent_dimension, 1),\n",
    "        ).squeeze()\n",
    "        relu_f = nn.ReLU()\n",
    "        return relu_f(inner_prod).squeeze()\n",
    "\n",
    "    def classify(self, molecule, protein):\n",
    "        molecule_projection = self.molecule_projector(molecule)\n",
    "        protein_projection = self.protein_projector(protein)\n",
    "\n",
    "        molecule_projection = molecule_projection.unsqueeze(0) \n",
    "        protein_projection = protein_projection.unsqueeze(1) \n",
    "\n",
    "        distance = self.activator(molecule_projection, protein_projection)\n",
    "        \n",
    "        scaled_distance = distance / self.temperature\n",
    "\n",
    "        return scaled_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb59ed3-989b-4b67-905a-caa8d907e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, dataframe, prot_tokenizer, prot_model, mol_tokenizer, mol_model, device):\n",
    "        self.data = dataframe\n",
    "        self.prot_tokenizer = prot_tokenizer\n",
    "        self.prot_model = prot_model.to(device)  # Move model to device\n",
    "        self.mol_tokenizer = mol_tokenizer\n",
    "        self.mol_model = mol_model.to(device)    # Move model to device\n",
    "        self.device = device\n",
    "        \n",
    "        # Freeze model parameters to save memory and computation\n",
    "        self.prot_model.eval()\n",
    "        self.mol_model.eval()\n",
    "        for param in self.prot_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.mol_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        anchor = row['sequence']\n",
    "        positive = row['canonicalsmiles']\n",
    "        return anchor, positive\n",
    "\n",
    "def generate_anchor_embeddings_batch(sequences, tokenizer, lora_model, device):\n",
    "    lora_model.to(device)\n",
    "    inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = lora_model.esm(**inputs).last_hidden_state\n",
    "        mean_output = output[:, 1:output.size(1)].mean(dim=1)\n",
    "    return mean_output.cpu()\n",
    "\n",
    "def generate_mol_embeddings_batch(smiles_list, tokenizer, mol_model, device, target_dim=1280):\n",
    "    try:\n",
    "        mol_model.to(device)\n",
    "        inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = mol_model(**inputs)\n",
    "            mol_embedding = outputs.pooler_output\n",
    "        return mol_embedding.cpu()  # Move to CPU only after computation\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SMILES: {smiles_list}, Error: {e}\")\n",
    "        return torch.zeros((len(smiles_list), target_dim))  # Return zero tensor for invalid SMILES\n",
    "\n",
    "def contrastive_collate_fn(batch, prot_tokenizer, prot_model, mol_tokenizer, mol_model, device):\n",
    "    anchors, positives = zip(*batch)\n",
    "    \n",
    "    # Batch process anchor sequences\n",
    "    anchor_embs = generate_anchor_embeddings_batch(\n",
    "        anchors, prot_tokenizer, prot_model, device\n",
    "    )\n",
    "    \n",
    "    # Batch process positive and negative SMILES\n",
    "    positive_embs = generate_mol_embeddings_batch(\n",
    "        positives, mol_tokenizer, mol_model, device\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        'anchorEmb': anchor_embs,\n",
    "        'positiveEmb': positive_embs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c403be-ddb4-472c-95a2-9ba656abcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, dataframe, prot_tokenizer, prot_model, device):\n",
    "        self.data = dataframe\n",
    "        self.prot_tokenizer = prot_tokenizer\n",
    "        self.prot_model = prot_model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        self.prot_model.eval()\n",
    "        for param in self.prot_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.index2label = {index: canonicalsmiles for index, canonicalsmiles in enumerate(self.data.canonicalsmiles.unique())}\n",
    "        self.label2index = {canonicalsmiles: index for index, canonicalsmiles in enumerate(self.data.canonicalsmiles.unique())}\n",
    "        self.data['num_labels'] = self.data.canonicalsmiles.map(self.label2index).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return row['sequence'], row[['num_labels']]\n",
    "    \n",
    "def classification_collate_fn(batch, prot_tokenizer, prot_model, device):\n",
    "    proteins, labels = zip(*batch)\n",
    "\n",
    "    prot_embs = generate_anchor_embeddings_batch(proteins, prot_tokenizer, prot_model, device)\n",
    "\n",
    "    return {\n",
    "        'prot_emb': prot_embs,\n",
    "        'labels': torch.tensor(labels, dtype=torch.long).squeeze(-1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1a6f6-b874-4bad-ac4c-31d838800646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation Function #\n",
    "\n",
    "def evaluate_model(model, mol_embs, test_loader, loss_fn, device):\n",
    "    model.eval() \n",
    "    test_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            prot_embs = batch['prot_emb'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(mol_embs, prot_embs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Get predictions and save\n",
    "            preds = torch.argmax(F.softmax(logits, dim=-1), dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate average test loss\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd90649-1f25-4352-9d85-675dd085dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Setup\n",
    "def setup_logging():\n",
    "    log_dir = 'training_log'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_filename = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "    logging.basicConfig(\n",
    "        filename=log_filename,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "        \"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger().addHandler(console)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffe055-1690-4623-bdbc-a44082537308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Script\n",
    "def main():\n",
    "    # Set up logging\n",
    "    setup_logging()\n",
    "    logging.info(\"Starting training process.\")\n",
    "\n",
    "    # Configuration parameters\n",
    "    config = {\n",
    "        'epochs': 100,\n",
    "        'T_0': 10,\n",
    "        'lr_triplet': 1e-4,  \n",
    "        'lr_ce': 1e-4,         \n",
    "        'latent_distance': 'Cosine'  \n",
    "    }\n",
    "\n",
    "    # Load classification data\n",
    "    classification_data_path = \"T2_data_normalized.xlsx\"\n",
    "    if not os.path.exists(classification_data_path):\n",
    "        logging.error(f\"Data file not found: {classification_data_path}\")\n",
    "        return\n",
    "    classification_df = pd.read_excel(classification_data_path)\n",
    "    classification_df = classification_df.drop_duplicates(subset = ['canonicalsmiles'])\n",
    "    \n",
    "    logging.info(f\"Successfully loaded classification data, sample size: {len(classification_df)}\")\n",
    "\n",
    "    # Load contrastive learning data\n",
    "    contrastive_data_path = \"T2_data_normalized.xlsx\"\n",
    "    if not os.path.exists(contrastive_data_path):\n",
    "        logging.error(f\"Data file not found: {contrastive_data_path}\")\n",
    "        return\n",
    "    contrastive_df = pd.read_excel(contrastive_data_path)\n",
    "    contrastive_df = contrastive_df.drop_duplicates(subset = ['canonicalsmiles'])\n",
    "    logging.info(f\"Successfully loaded contrastive learning data, sample size: {len(contrastive_df)}\")\n",
    "\n",
    "    # Load protein model and tokenizer\n",
    "    model_name = 'esm2/esm2_t33_650M_UR50D'\n",
    "    prot_tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    base_model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "    prot_model = PeftModel.from_pretrained(base_model, './plm')\n",
    "    logging.info(\"Successfully loaded protein model and tokenizer.\")\n",
    "\n",
    "    # Load molecule model and tokenizer\n",
    "    mol_model_path = \"./ibm/MoLFormer-XL-both-10pct\"\n",
    "    mol_tokenizer = AutoTokenizer.from_pretrained(mol_model_path, trust_remote_code=True)\n",
    "    mol_model = AutoModel.from_pretrained(mol_model_path, deterministic_eval=True, trust_remote_code=True)\n",
    "    logging.info(\"Successfully loaded molecule model and tokenizer.\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model\n",
    "    protein_embedding_dim = 1280\n",
    "    molecule_embedding_dim = 768\n",
    "    projection_dim = 1024\n",
    "\n",
    "    model = Coembedding(\n",
    "        molecule_shape=molecule_embedding_dim,\n",
    "        protein_shape=protein_embedding_dim,\n",
    "        latent_dimension=projection_dim,\n",
    "        latent_activation=nn.ReLU,\n",
    "        latent_distance=config.get('latent_distance', \"Cosine\"),\n",
    "        classify=True\n",
    "    ).to(device)\n",
    "    logging.info(\"Successfully initialized co-embedding model.\")\n",
    "\n",
    "    # Create ClassificationDataset and ContrastiveDataset instances\n",
    "    classification_dataset = ClassificationDataset(\n",
    "        dataframe=classification_df,\n",
    "        prot_tokenizer=prot_tokenizer,\n",
    "        prot_model=prot_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    contrastive_dataset = ContrastiveDataset(\n",
    "        dataframe=contrastive_df,\n",
    "        prot_tokenizer=prot_tokenizer,\n",
    "        prot_model=prot_model,\n",
    "        mol_tokenizer=mol_tokenizer,\n",
    "        mol_model=mol_model,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    logging.info(f\"ClassificationDataset and ContrastiveDataset creation completed\")\n",
    "\n",
    "    # Split classification dataset into training and test sets\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        list(range(len(classification_dataset))),\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_classification_dataset = Subset(classification_dataset, train_indices)\n",
    "    test_classification_dataset = Subset(classification_dataset, test_indices)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    batch_size = 32\n",
    "\n",
    "    # DataLoader for classification task\n",
    "    train_classification_loader = DataLoader(\n",
    "        train_classification_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: classification_collate_fn(batch, prot_tokenizer, prot_model, device),\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_classification_loader = DataLoader(\n",
    "        test_classification_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: classification_collate_fn(batch, prot_tokenizer, prot_model, device),\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    mol_embs = generate_mol_embeddings_batch(list(classification_dataset.index2label.values()), mol_tokenizer, mol_model, device).to(device)\n",
    "\n",
    "    # DataLoader for contrastive learning task\n",
    "    contrastive_loader = DataLoader(\n",
    "        contrastive_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: contrastive_collate_fn(batch, prot_tokenizer, prot_model, mol_tokenizer, mol_model, device),\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    logging.info(f\"DataLoader creation completed, batch size: {batch_size}\")\n",
    "\n",
    "    # Define optimizers and schedulers for both loss functions\n",
    "    contrastive_opt = torch.optim.AdamW(model.parameters(), lr=config['lr_triplet'])\n",
    "    contrastive_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(contrastive_opt, T_0=config['T_0'])\n",
    "\n",
    "    classfication_opt = torch.optim.AdamW(model.parameters(), lr=config['lr_ce'])\n",
    "    classfication_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(classfication_opt, T_0=config['T_0'])\n",
    "\n",
    "    # Define loss functions\n",
    "    loss_fn = ContrastiveLoss()\n",
    "    classfication_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    logging.info(\"Optimizers, schedulers and loss functions initialization completed.\")\n",
    "\n",
    "    os.makedirs('model_weight', exist_ok=True)\n",
    "\n",
    "    best_test_acc = 0.0 \n",
    "    best_epoch = 0 \n",
    "\n",
    "    for epo in range(config['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Contrastive learning training\n",
    "        for batch_idx, batch in enumerate(tqdm(contrastive_loader, total=len(contrastive_loader), desc=f\"Contrastive Epoch {epo+1}/{config['epochs']}\")):\n",
    "            anchor = batch['anchorEmb'].to(device)\n",
    "            positive = batch['positiveEmb'].to(device)\n",
    "        \n",
    "            anchor_projection = F.normalize(model.protein_projector(anchor), p=2, dim=1)\n",
    "            positive_projection = F.normalize(model.molecule_projector(positive), p=2, dim=1)\n",
    "        \n",
    "            loss = loss_fn(anchor_projection, positive_projection)\n",
    "\n",
    "            contrastive_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            contrastive_opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_contrastive_loss = total_loss / len(contrastive_loader)\n",
    "        contrastive_scheduler.step()\n",
    "\n",
    "        logging.info(f\"Contrastive Epoch {epo+1}/{config['epochs']}, Loss: {avg_contrastive_loss:.4f}\")\n",
    "\n",
    "        # Classification task training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_classification_loader, total=len(train_classification_loader), desc=f\"Classification Epoch {epo+1}/{config['epochs']}\")):\n",
    "            prot_embs = batch['prot_emb'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(mol_embs, prot_embs)    \n",
    "            loss = classfication_fn(logits, labels)\n",
    "\n",
    "            classfication_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            classfication_opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_classification_loss = total_loss / len(train_classification_loader)\n",
    "        classfication_scheduler.step()\n",
    "\n",
    "        logging.info(f\"Classification Epoch {epo+1}/{config['epochs']}, Loss: {avg_classification_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate the model on the training set\n",
    "        avg_train_loss, train_acc = evaluate_model(model, mol_embs, train_classification_loader, classfication_fn, device)\n",
    "        logging.info(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        avg_test_loss, test_acc = evaluate_model(model, mol_embs, test_classification_loader, classfication_fn, device)\n",
    "        logging.info(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # Save the model if test accuracy improves\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_epoch = epo + 1\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epo + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'contrastive_opt_state_dict': contrastive_opt.state_dict(),\n",
    "                'contrastive_scheduler_state_dict': contrastive_scheduler.state_dict(),\n",
    "                'classfication_opt_state_dict': classfication_opt.state_dict(),\n",
    "                'classfication_scheduler_state_dict': classfication_scheduler.state_dict(),\n",
    "                'loss': avg_test_loss,\n",
    "                'accuracy': best_test_acc,\n",
    "            }, 'model_weight/best_model.pth')\n",
    "\n",
    "            logging.info(f\"New best model saved at epoch {best_epoch} with test accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "    logging.info(f\"Training completed. Best test accuracy: {best_test_acc:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797e947-654f-42b7-8308-983d3ee1cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'esm2/esm2_t33_650M_UR50D'\n",
    "prot_tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "base_model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "prot_model = PeftModel.from_pretrained(base_model, './plm')\n",
    "\n",
    "mol_model_path = \"./ibm/MoLFormer-XL-both-10pct\"\n",
    "mol_tokenizer = AutoTokenizer.from_pretrained(mol_model_path, trust_remote_code=True)\n",
    "mol_model = AutoModel.from_pretrained(mol_model_path, deterministic_eval=True, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = pd.read_excel(\"T2_data_normalized.xlsx\")\n",
    "data = data.drop_duplicates(subset = ['canonicalsmiles'])\n",
    "\n",
    "# prot_seq = data.sequence.tolist()\n",
    "prot_seq = test_data.sequence.tolist()\n",
    "mol_smiles = data.canonicalsmiles.tolist()\n",
    "\n",
    "unique_labels = data.T2PKproductsname.tolist()\n",
    "label_to_index = {productsname: idx for idx, productsname in enumerate(unique_labels)}\n",
    "index_to_label = {idx: productsname for productsname, idx in label_to_index.items()}\n",
    "# true_labels = data['T2PKproductsname'].map(label_to_index).tolist()\n",
    "true_labels = test_data['T2PKproductsname'].map(label_to_index).tolist()\n",
    "\n",
    "prot_emb = generate_anchor_embeddings_batch(\n",
    "        prot_seq, prot_tokenizer, prot_model, device\n",
    "    )\n",
    "\n",
    "mol_emb = generate_mol_embeddings_batch(\n",
    "        mol_smiles, mol_tokenizer, mol_model, device\n",
    "    )\n",
    "\n",
    "model = Coembedding().to(device)\n",
    "model.load_state_dict(torch.load('model_weight/best_model_1024_ce_triplet_1022_final.pth')['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prot_emb = prot_emb.to(device)\n",
    "    mol_emb = mol_emb.to(device)\n",
    "    prediction = torch.argmax(model(mol_emb, prot_emb), dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73664eef-74fa-4c59-a4f4-b1c032f2baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = prediction.cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "precision = precision_score(true_labels, pred_labels, average='weighted')\n",
    "recall = recall_score(true_labels, pred_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
