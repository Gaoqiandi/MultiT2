{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49373ff8-1e1a-4820-b0da-50a2afa1558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import (\n",
    "    EsmTokenizer,\n",
    "    EsmForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel\n",
    ")\n",
    "from peft import PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9cd66a-5aa9-4472-b227-6e45b3518c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Definition\n",
    "class Cosine(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return nn.CosineSimilarity(dim=-1)(x1, x2)\n",
    "\n",
    "class SquaredCosine(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return nn.CosineSimilarity(dim=-1)(x1, x2) ** 2\n",
    "\n",
    "class Euclidean(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cdist(x1, x2, p=2.0)\n",
    "\n",
    "class SquaredEuclidean(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cdist(x1, x2, p=2.0) ** 2\n",
    "\n",
    "DISTANCE_METRICS = {\n",
    "    \"Cosine\": Cosine,\n",
    "    \"SquaredCosine\": SquaredCosine,\n",
    "    \"Euclidean\": Euclidean,\n",
    "    \"SquaredEuclidean\": SquaredEuclidean,\n",
    "}\n",
    "\n",
    "class Coembedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        molecule_shape: int = 768,\n",
    "        protein_shape: int = 1280,\n",
    "        latent_dimension: int = 1024,\n",
    "        latent_activation=nn.ReLU,\n",
    "        latent_distance: str = \"Cosine\",\n",
    "        classify: bool = True,\n",
    "        temperature: float = 0.1\n",
    "    ):\n",
    "        super(Coembedding, self).__init__()\n",
    "        self.molecule_shape = molecule_shape\n",
    "        self.protein_shape = protein_shape\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.do_classify = classify\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "\n",
    "        self.molecule_projector = nn.Sequential(\n",
    "            nn.Linear(self.molecule_shape, latent_dimension),\n",
    "            latent_activation(),\n",
    "            nn.Linear(latent_dimension, latent_dimension)\n",
    "        )\n",
    "        \n",
    "        for layer in self.molecule_projector:\n",
    "            if isinstance(layer, nn.Linear): \n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "        self.protein_projector = nn.Sequential(\n",
    "            nn.Linear(self.protein_shape, latent_dimension),\n",
    "            latent_activation(),\n",
    "            nn.Linear(latent_dimension, latent_dimension)\n",
    "        )\n",
    "        \n",
    "        for layer in self.protein_projector:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "\n",
    "        if self.do_classify:\n",
    "            if latent_distance not in DISTANCE_METRICS:\n",
    "                raise ValueError(f\"Unsupported distance metric: {latent_distance}\")\n",
    "            self.distance_metric = latent_distance\n",
    "            self.activator = DISTANCE_METRICS[self.distance_metric]()\n",
    "\n",
    "    def forward(self, molecule, protein):\n",
    "        if self.do_classify:\n",
    "            return self.classify(molecule, protein)\n",
    "        else:\n",
    "            return self.regress(molecule, protein)\n",
    "\n",
    "    def regress(self, molecule, protein):\n",
    "        molecule_projection = self.molecule_projector(molecule)\n",
    "        protein_projection = self.protein_projector(protein)\n",
    "\n",
    "        inner_prod = torch.bmm(\n",
    "            molecule_projection.view(-1, 1, self.latent_dimension),\n",
    "            protein_projection.view(-1, self.latent_dimension, 1),\n",
    "        ).squeeze()\n",
    "        relu_f = nn.ReLU()\n",
    "        return relu_f(inner_prod).squeeze()\n",
    "\n",
    "    def classify(self, molecule, protein):\n",
    "        molecule_projection = self.molecule_projector(molecule)\n",
    "        protein_projection = self.protein_projector(protein)\n",
    "\n",
    "        molecule_projection = molecule_projection.unsqueeze(0) \n",
    "        protein_projection = protein_projection.unsqueeze(1) \n",
    "\n",
    "        distance = self.activator(molecule_projection, protein_projection)\n",
    "        \n",
    "        scaled_distance = distance / self.temperature\n",
    "\n",
    "        return scaled_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b95d6-2798-4c7b-9dd3-a30fdea31424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_embeddings_batch(sequences, tokenizer, lora_model, device):\n",
    "    lora_model.to(device)\n",
    "    inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = lora_model.esm(**inputs).last_hidden_state\n",
    "        mean_output = output[:, 1:output.size(1)].mean(dim=1)\n",
    "    return mean_output.cpu()\n",
    "\n",
    "def generate_mol_embeddings_batch(smiles_list, tokenizer, mol_model, device, target_dim=1280):\n",
    "    try:\n",
    "        mol_model.to(device)\n",
    "        inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = mol_model(**inputs)\n",
    "            mol_embedding = outputs.pooler_output\n",
    "        return mol_embedding.cpu()  # Move to CPU only after computation\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SMILES: {smiles_list}, Error: {e}\")\n",
    "        return torch.zeros((len(smiles_list), target_dim))  # Return zero tensor for invalid SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bdeedc-1efe-4455-a0e4-008ffef8ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'esm2/esm2_t33_650M_UR50D'\n",
    "prot_tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "base_model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "prot_model = PeftModel.from_pretrained(base_model, './plm')\n",
    "\n",
    "mol_model_path = \"./ibm/MoLFormer-XL-both-10pct\"\n",
    "mol_tokenizer = AutoTokenizer.from_pretrained(mol_model_path, trust_remote_code=True)\n",
    "mol_model = AutoModel.from_pretrained(mol_model_path, deterministic_eval=True, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Read protein sequences from FASTA file\n",
    "fasta_file = \"test.fasta\"\n",
    "prot_seq = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "# Read molecule data\n",
    "data = pd.read_excel(\"T2_data_normalized.xlsx\")\n",
    "mol_smiles = data.canonicalsmiles.tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "prot_emb = generate_anchor_embeddings_batch(prot_seq, prot_tokenizer, prot_model, device)\n",
    "mol_emb = generate_mol_embeddings_batch(mol_smiles, mol_tokenizer, mol_model, device)\n",
    "\n",
    "# Initialize and load model\n",
    "model = Coembedding(molecule_shape=768,\n",
    "                    protein_shape=1280,\n",
    "                    latent_dimension=1024,\n",
    "                    latent_activation=nn.ReLU,\n",
    "                    latent_distance=\"Cosine\",\n",
    "                    classify=True,\n",
    "                    temperature=0.1).to(device)\n",
    "\n",
    "checkpoint = torch.load('model_weight/best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "with torch.no_grad():\n",
    "    prot_emb = prot_emb.to(device)\n",
    "    mol_emb = mol_emb.to(device)\n",
    "    similarity_matrix = model(mol_emb, prot_emb)\n",
    "\n",
    "# Apply softmax normalization to similarity matrix\n",
    "similarity_matrix_softmax = F.softmax(similarity_matrix, dim=1)\n",
    "\n",
    "# Get indices and similarities of top 5 molecules for each protein\n",
    "top_indices = torch.argsort(-similarity_matrix_softmax, dim=1)[:, :5]\n",
    "top_similarities = torch.gather(-similarity_matrix_softmax, 1, top_indices)\n",
    "\n",
    "df = pd.read_excel(\"T2_data_normalized.xlsx\")\n",
    "name_to_smiles = dict(zip(data['canonicalsmiles'], data['T2PKproductsname']))\n",
    "\n",
    "# Output top 3 similar molecules for each protein\n",
    "for i, (indices, similarities) in enumerate(zip(top_indices.cpu().numpy(), top_similarities.cpu().numpy())):\n",
    "    print(f\"Protein {i} top 3 similar molecules:\")\n",
    "    for index, similarity in zip(indices, similarities):\n",
    "        smiles = mol_smiles[index]\n",
    "        molecule_label = name_to_smiles.get(smiles, \"Unknown\")\n",
    "        print(f\"  - Molecule Label: {molecule_label}, SMILES: {smiles}, Similarity: {-similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESM (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
